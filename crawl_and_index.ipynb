{"cells":[{"cell_type":"code","execution_count":11,"id":"4392baa5","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["#IMPORTS\n","!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes\n","\n","import json\n","import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *\n","\n"]},{"cell_type":"code","execution_count":37,"id":"2a7bd087","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["bucket_name = '340915149wiki' \n","client = storage.Client()\n","full_path = \"gs://wikidata_preprocessed/*\"\n","parquetFile = spark.read.parquet(full_path)\n","#CHANGE THIS\n","# wiki1000_body = parquetFile.select(\"id\", \"text\").rdd\n","wiki1000_title = parquetFile.select(\"id\", \"title\").rdd\n","# wiki1000_anchor = parquetFile.select(\"id\", \"anchor_text\").rdd"]},{"cell_type":"markdown","id":"f1643f23","metadata":{},"source":["ID TO TITLES"]},{"cell_type":"code","execution_count":9,"id":"2ae6154c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["ids_titles = wiki1000_title.collectAsMap()"]},{"cell_type":"code","execution_count":7,"id":"776b4e9e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://titles.json [Content-Type=application/json]...\n","/ [1 files][ 33.5 KiB/ 33.5 KiB]                                                \n","Operation completed over 1 objects/33.5 KiB.                                     \n"]}],"source":["with open('titles.json', 'w') as titles:\n","     json.dump(ids_titles, titles)\n","\n","titles_src = \"titles.json\"\n","titles_dst = f'gs://{bucket_name}/titles/{titles_src}'\n","!gsutil cp $titles_src $titles_dst"]},{"cell_type":"code","execution_count":13,"id":"e24b6b60","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index.py\r\n"]}],"source":["%cd -q /home/dataproc\n","!ls inverted_index.py\n","sc.addFile(\"/home/dataproc/inverted_index.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())\n","from inverted_index import InvertedIndex"]},{"cell_type":"code","execution_count":14,"id":"1284b91c","metadata":{},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","ps = PorterStemmer()"]},{"cell_type":"code","execution_count":6,"id":"64ff4d8a","metadata":{},"outputs":[],"source":["def get_tf(id, text):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    # filtered tokens\n","    tokens = [token for token in tokens if token not in all_stopwords]\n","    # use counter to count and map values with id \n","    return list(map(lambda x: (x[0], (id, x[1])), list(Counter(tokens).items())))"]},{"cell_type":"code","execution_count":7,"id":"718750a9","metadata":{},"outputs":[],"source":["def get_DL(text):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    # filtered tokens\n","    tokens = [token for token in tokens if token not in all_stopwords]\n","    return len(tokens)"]},{"cell_type":"markdown","id":"a669e398","metadata":{},"source":["BODY"]},{"cell_type":"code","execution_count":8,"id":"539a23e9","metadata":{},"outputs":[],"source":["#get tfs\n","body_tfs = wiki1000_body.flatMap(lambda x: get_tf(x[0], x[1]))"]},{"cell_type":"code","execution_count":9,"id":"4fcb1be1","metadata":{},"outputs":[],"source":["#sort pls\n","body_postings = body_tfs.groupByKey().mapValues(InvertedIndex.reduce_word_counts)\n","body_postings_filtered = body_postings.filter(lambda x: len(x[1])>50)"]},{"cell_type":"code","execution_count":10,"id":"8c8bc546","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["body_df = InvertedIndex.calculate_df(body_postings_filtered)\n","body_df_dict = body_df.collectAsMap()"]},{"cell_type":"code","execution_count":11,"id":"8102bf5d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["body_DL = wiki1000_body.map(lambda x: (x[0], get_DL(x[1]))).collectAsMap()"]},{"cell_type":"code","execution_count":null,"id":"57394c63","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["body_tot_term = body_postings_filtered.mapValues(InvertedIndex.get_total_term).collectAsMap()"]},{"cell_type":"code","execution_count":13,"id":"351444d8","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["body_posting_locs_list = InvertedIndex.partition_postings_and_write(body_postings_filtered, bucket_name, \"body\").collect()"]},{"cell_type":"code","execution_count":14,"id":"74616980","metadata":{},"outputs":[],"source":["body_super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='body'):\n","    if not blob.name.endswith(\"pickle\"):\n","        continue\n","    with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","            body_super_posting_locs[k].extend(v)"]},{"cell_type":"code","execution_count":15,"id":"51980216","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://body_index.pkl [Content-Type=application/octet-stream]...\n","- [1 files][ 69.0 MiB/ 69.0 MiB]                                                \n","Operation completed over 1 objects/69.0 MiB.                                     \n"]}],"source":["# Create inverted index instance\n","body_inv_index = InvertedIndex()\n","body_inv_index.DL = body_DL\n","# Adding the posting locations dictionary to the inverted index\n","body_inv_index.posting_locs = body_super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","body_inv_index.df = body_df_dict\n","body_inv_index.term_total = body_tot_term\n","# write the global stats out\n","body_inv_index.write_index('.', 'body_index')\n","index_src = \"body_index.pkl\"\n","index_dst = f'gs://{bucket_name}/body/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":null,"id":"78bd8cbb","metadata":{},"outputs":[],"source":["# body_inv_index.read_posting_list('state', 'body', bucket_name)"]},{"cell_type":"markdown","id":"47801f3c","metadata":{},"source":["TITLE"]},{"cell_type":"code","execution_count":38,"id":"6d2e01c2","metadata":{},"outputs":[],"source":["def get_title_tf(id, text):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    # filtered tokens\n","    tokens = [token for token in tokens if token not in all_stopwords]\n","    # use counter to count and map values with id \n","    stems = []\n","    for token in tokens:\n","        stems.append(ps.stem(token))\n","    return list(map(lambda x: (x[0], (id, x[1])), list(Counter(stems).items())))\n","\n","def get_title_DL(text):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    # filtered tokens\n","    tokens = [token for token in tokens if token not in all_stopwords]\n","    stems = []\n","    for token in tokens:\n","        stems.append(ps.stem(token))\n","    return len(stems) # TODO Maybe not filtered\n"]},{"cell_type":"code","execution_count":39,"id":"e6cb106f","metadata":{},"outputs":[],"source":["#get tfs\n","title_tfs = wiki1000_title.flatMap(lambda x: get_title_tf(x[0], x[1]))"]},{"cell_type":"code","execution_count":40,"id":"c1aea495","metadata":{},"outputs":[],"source":["# #sort pls\n","title_postings = title_tfs.groupByKey().mapValues(InvertedIndex.reduce_word_counts)"]},{"cell_type":"code","execution_count":41,"id":"04dade65","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["title_df = InvertedIndex.calculate_df(title_postings)\n","title_df_dict = title_df.collectAsMap()"]},{"cell_type":"code","execution_count":42,"id":"3dbafe7d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["title_DL = wiki1000_title.map(lambda x: (x[0], get_title_DL(x[1]))).collectAsMap()"]},{"cell_type":"code","execution_count":43,"id":"0dac7b85","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["title_tot_term = title_tfs.groupByKey().mapValues(InvertedIndex.get_total_term).collectAsMap()"]},{"cell_type":"code","execution_count":44,"id":"6cccb357","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["title_posting_locs_list = InvertedIndex.partition_postings_and_write(title_postings, bucket_name, \"title\").collect()"]},{"cell_type":"code","execution_count":45,"id":"4b6a1b2b","metadata":{},"outputs":[],"source":["title_super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='title'):\n","    if not blob.name.endswith(\"pickle\"):\n","        continue\n","    with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","            title_super_posting_locs[k].extend(v)"]},{"cell_type":"code","execution_count":46,"id":"cf41d5a2","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://title_index.pkl [Content-Type=application/octet-stream]...\n","| [1 files][126.1 MiB/126.1 MiB]                                                \n","Operation completed over 1 objects/126.1 MiB.                                    \n"]}],"source":["# Create inverted index instance\n","title_inv_index = InvertedIndex()\n","title_inv_index.DL = title_DL\n","# Adding the posting locations dictionary to the inverted index\n","title_inv_index.posting_locs = title_super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","title_inv_index.df = title_df_dict\n","title_inv_index.term_total = title_tot_term\n","# write the global stats out\n","title_inv_index.write_index('.', 'title_index')\n","index_src2 = \"title_index.pkl\"\n","index_dst2 = f'gs://{bucket_name}/title/{index_src2}'\n","!gsutil cp $index_src2 $index_dst2"]},{"cell_type":"code","execution_count":47,"id":"46d310ef","metadata":{},"outputs":[{"data":{"text/plain":["[]"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["title_inv_index.read_posting_list('anarchism', 'title', bucket_name)"]},{"cell_type":"code","execution_count":null,"id":"0e50db6b","metadata":{},"outputs":[],"source":["title_inv_index.read_posting_list('anarch', 'title', bucket_name)"]},{"cell_type":"markdown","id":"b5ec6a46","metadata":{},"source":["ANCHOR"]},{"cell_type":"code","execution_count":20,"id":"6cee4df5","metadata":{},"outputs":[],"source":["def get_anchor_tf(id, rows):\n","    rows2 = list(map(lambda x: x[1], rows))\n","    text = ''\n","    for t in rows2:\n","        text+=t+\" \"\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    # filtered tokens\n","    tokens = [token for token in tokens if token not in all_stopwords]\n","    # use counter to count and map values with id \n","    return list(map(lambda x: (x[0], (id, x[1])), list(Counter(tokens).items())))"]},{"cell_type":"code","execution_count":21,"id":"d5e60e25","metadata":{},"outputs":[],"source":["def get_anchor_DL(rows):\n","    rows2 = list(map(lambda x: x[1], rows))\n","    text = ''\n","    for t in rows2:\n","        text+=t+\" \"\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    # filtered tokens\n","    tokens = [token for token in tokens if token not in all_stopwords]\n","    return len(tokens) # TODO Maybe not filtered"]},{"cell_type":"code","execution_count":22,"id":"2e8ce9e1","metadata":{},"outputs":[],"source":["#get tfs\n","anchor_tfs = wiki1000_anchor.flatMap(lambda x: get_anchor_tf(x[0], x[1]))"]},{"cell_type":"code","execution_count":23,"id":"35c3dc9a","metadata":{},"outputs":[],"source":["# sort pls\n","anchor_postings = anchor_tfs.groupByKey().mapValues(InvertedIndex.reduce_word_counts)\n","anchor_postings_filtered = body_postings.filter(lambda x: len(x[1])>20)"]},{"cell_type":"code","execution_count":24,"id":"1c16375d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["anchor_df = InvertedIndex.calculate_df(anchor_postings_filtered)\n","anchor_df_dict = anchor_df.collectAsMap()"]},{"cell_type":"code","execution_count":25,"id":"46100c9d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["anchor_DL = wiki1000_anchor.map(lambda x: (x[0], get_anchor_DL(x[1]))).collectAsMap()"]},{"cell_type":"code","execution_count":26,"id":"b2de3b6c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["anchor_tot_term = anchor_postings_filtered.mapValues(InvertedIndex.get_total_term).collectAsMap()"]},{"cell_type":"code","execution_count":27,"id":"4b7b1e21","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["anchor_posting_locs_list = InvertedIndex.partition_postings_and_write(anchor_postings_filtered, bucket_name, \"anchor\").collect()"]},{"cell_type":"code","execution_count":28,"id":"05b6581d","metadata":{},"outputs":[],"source":["anchor_super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='anchor'):\n","    if not blob.name.endswith(\"pickle\"):\n","        continue\n","    with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","            anchor_super_posting_locs[k].extend(v)"]},{"cell_type":"code","execution_count":30,"id":"adedf085","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://anchor_index.pkl [Content-Type=application/octet-stream]...\n","- [1 files][ 91.4 MiB/ 91.4 MiB]                                                \n","Operation completed over 1 objects/91.4 MiB.                                     \n"]}],"source":["# Create inverted index instance\n","anchor_inv_index = InvertedIndex()\n","anchor_inv_index.DL = anchor_DL\n","# Adding the posting locations dictionary to the inverted index\n","anchor_inv_index.posting_locs = anchor_super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","anchor_inv_index.df = anchor_df_dict\n","anchor_inv_index.term_total = anchor_tot_term\n","# write the global stats out\n","anchor_inv_index.write_index('.', 'anchor_index')\n","index_src3 = \"anchor_index.pkl\"\n","index_dst3 = f'gs://{bucket_name}/anchor/{index_src3}'\n","!gsutil cp $index_src3 $index_dst3"]},{"cell_type":"code","execution_count":null,"id":"0b850976","metadata":{},"outputs":[],"source":["# anchor_inv_index.read_posting_list('teresa', 'anchor', bucket_name)"]},{"cell_type":"markdown","id":"8c197396","metadata":{},"source":["PAGE RANK"]},{"cell_type":"code","execution_count":null,"id":"3d0bbb00","metadata":{},"outputs":[],"source":["def get_ids_from_anchors(id,anchorlist):\n","    ids = []\n","    for anchor in anchorlist:\n","        ids.append((id, anchor[0]))\n","    return ids\n","  \n","def generate_graph(pages):\n","    verticesFromLinks = pages.flatMap(lambda x: x[1]).map(lambda x: x[0])\n","    verticeFromIds = pages.map(lambda x: x[0])\n","    vertices = verticeFromIds.union(verticesFromLinks).distinct().map(lambda x: (x, ))\n","  \n","    edges = pages.flatMap( lambda x: (get_ids_from_anchors(x[0],x[1]))).distinct()\n","\n","    return edges, vertices"]},{"cell_type":"code","execution_count":null,"id":"4b5de070","metadata":{},"outputs":[],"source":["edges, vertices = generate_graph(wiki1000_anchor)\n","v_cnt, e_cnt = vertices.count(), edges.count()"]},{"cell_type":"code","execution_count":null,"id":"ead28624","metadata":{},"outputs":[],"source":["edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","# pr = pr.sort(col('pagerank').desc())\n","# pr.repartition(1).write.csv(f'gs://{bucket_name}/pr', compression=\"gzip\")\n","# pr.show()\n","\n","# dictpr - dictionary: (key: doc_id, value: doc_pagerank)\n","dictpr = {}\n","for row in pr.toPandas().iterrows():\n","    dictpr[int(row[1][0])]=row[1][1]\n","\n","with open('pr.json', 'w') as pr:\n","     json.dump(dictpr, pr)\n","\n","pr_src = \"pr.json\"\n","pr_dst = f'gs://{bucket_name}/pr/{pr_src}'\n","!gsutil cp $pr_src $pr_dst"]},{"cell_type":"markdown","id":"894de124","metadata":{},"source":["PAGE VIEWS"]},{"cell_type":"code","execution_count":null,"id":"c579b4c9","metadata":{},"outputs":[],"source":["pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n","p = Path(pv_path) \n","pv_name = p.name\n","pv_temp = f'{p.stem}-4dedup.txt'\n","pv_clean = f'{p.stem}.pkl'\n","!wget -N $pv_path\n","!bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n","wid2pv = Counter()\n","with open(pv_temp, 'rt') as f:\n","    for line in f:\n","        parts = line.split(' ')\n","        wid2pv.update({int(parts[0]): int(parts[1])})"]},{"cell_type":"code","execution_count":null,"id":"607ead74","metadata":{},"outputs":[],"source":["with open('pv.json', 'w') as pv:\n","     json.dump(wid2pv, pv)\n","\n","pv_src = \"pv.json\"\n","pv_dst = f'gs://{bucket_name}/pv/{pv_src}'\n","!gsutil cp $pv_src $pv_dst"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":5}
